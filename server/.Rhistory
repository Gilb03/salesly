library(geojson)
library(geojsonio)
library(htmlwidgets)
library(RColorBrewer)
library(tidyverse)
library(leaflet)
knitr::opts_chunk$set(echo = TRUE)
library(geojson)
library(geojsonio)
# define a string that represents the place we from where we will get the data
url <- "https://leafletjs.com/examples/choropleth/us-states.js"
# read as text file with the readLines function
doc <- readLines(url)
# remove the javascript assignment at the front of the doc
doc2 <- gsub("var statesData = ", "", doc)
# write out as a temp file and read
write(doc2, file = "tempgeo.json")
states <- geojson_read("tempgeo.json",method = "web", what = "sp")
class(states)
names(states)
library(leaflet)
library(htmlwidgets)
library(tidyverse)
m <- leaflet(states) %>%
setView(-96, 37.8, 4) %>%
addProviderTiles("CartoDB")
m %>% addPolygons()
library(RColorBrewer)
bins <- c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)
pal <- colorBin("YlOrRd", domain = states$density, bins = bins)
m %>% addPolygons(
fillColor = ~pal(density),
weight = 2,
opacity = 1,
color = "white",
dashArray = "3",
fillOpacity = 0.7)
m %>% addPolygons(
fillColor = ~pal(density),
weight = 2,
opacity = 1,
color = "white",
dashArray = "3",
fillOpacity = 0.7,
highlight = highlightOptions(
weight = 5,
color = "#666",
dashArray = "",
fillOpacity = 0.7,
bringToFront = TRUE))
labels <- sprintf(
"<strong>%s</strong><br/>%g people / mi<sup>2</sup>",
states$name, states$density
) %>% lapply(htmltools::HTML)
m <- m %>% addPolygons(
fillColor = ~pal(density),
weight = 2,
opacity = 1,
color = "white",
dashArray = "3",
fillOpacity = 0.7,
highlight = highlightOptions(
weight = 5,
color = "#666",
dashArray = "",
fillOpacity = 0.7,
bringToFront = TRUE),
label = labels,
labelOptions = labelOptions(
style = list("font-weight" = "normal", padding = "3px 8px"),
textsize = "15px",
direction = "auto"))
library(knitr)
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
options(width = 100, dplyr.width = 100)
library(ggplot2)
theme_set(theme_light())
text <- c("Because I could not stop for Death -",
"He kindly stopped for me -",
"The Carriage held but just Ourselves -",
"and Immortality")
text
library(dplyr)
text_df <- tibble(line = 1:4, text = text)
text_df
library(tidytext)
install.packages("tidytext")
library(tidytext)
text_df %>%
unnest_tokens(word, text)
knitr::include_graphics("images/tidyflow-ch-1.png")
install.packages("tidyflow")
knitr::include_graphics("images/tidyflow-ch-1.png")
library(janeaustenr)
library(dplyr)
library(stringr)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
original_books
library(tidytext)
tidy_books <- original_books %>%
unnest_tokens(word, text)
tidy_books
data(stop_words)
tidy_books <- tidy_books %>%
anti_join(stop_words)
tidy_books %>%
count(word, sort = TRUE)
library(ggplot2)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
library(gutenbergr)
install.packages("gutenbergr")
library(gutenbergr)
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
write_csv(hgwells,"hgwells.csv")
class(hgwells)
View(hgwells)
library(gutenbergr)
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
write_csv(hgwells,"hgwells.csv")
class(hgwells)
View(hgwells)
tidy_hgwells <- hgwells %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_hgwells %>%
count(word, sort = TRUE)
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
load("data/bronte.rda")
install.packages("textdata")
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
load("data/bronte.rda")
tidy_bronte <- bronte %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_bronte %>%
count(word, sort = TRUE)
library(tidyr)
frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
mutate(tidy_hgwells, author = "H.G. Wells"),
mutate(tidy_books, author = "Jane Austen")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)
library(scales)
# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
~ proportion + `Jane Austen`)
cor.test(data = frequency[frequency$author == "H.G. Wells",],
~ proportion + `Jane Austen`)
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
options(width = 100, dplyr.width = 100)
library(ggplot2)
theme_set(theme_light())
knitr::include_graphics("images/tidyflow-ch-2.png")
install.packages("wordcloud")
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
options(width = 100, dplyr.width = 100)
library(ggplot2)
theme_set(theme_light())
library(tidytext)
afinn_data <-get_sentiments("afinn")
View(afinn_data)
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
options(width = 100, dplyr.width = 100)
library(ggplot2)
theme_set(theme_light())
bing_data <- get_sentiments("bing")
View(bing_data)
nrc_data <- get_sentiments("nrc")
View(nrc_data)
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
library(tidyr)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
library(ggplot2)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
pride_prejudice <- tidy_books %>%
filter(book == "Pride & Prejudice")
pride_prejudice
afinn <- pride_prejudice %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index = linenumber %/% 80) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
bing_and_nrc <- bind_rows(pride_prejudice %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al."),
pride_prejudice %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative"))) %>%
mutate(method = "NRC")) %>%
count(method, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
afinn <- pride_prejudice %>%
inner_join(afinn_data) %>%
group_by(index = linenumber %/% 80) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
bing_and_nrc <- bind_rows(pride_prejudice %>%
inner_join(bing_data) %>%
mutate(method = "Bing et al."),
pride_prejudice %>%
inner_join(nrc_data %>%
filter(sentiment %in% c("positive",
"negative"))) %>%
mutate(method = "NRC")) %>%
count(method, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
bind_rows(afinn,
bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative")) %>%
count(sentiment) %>%
mutate(percentage = n/sum(n))
get_sentiments("bing") %>%
count(sentiment)%>%
mutate(percentage = n/sum(n))
bing_word_counts <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
bing_word_counts %>%
group_by(sentiment) %>%
filter(!word == "miss") %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
library(wordcloud)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
filter(!word == "miss") %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("orange", "blue"),
max.words = 100)
nrc_data %>%
group_by(sentiment) %>%
summarise(n=n()) %>%
mutate(percent = n/sum(n)) %>%
arrange(n)
bing_data %>%
group_by(sentiment) %>%
summarise(n=n()) %>%
mutate(percent = n/sum(n)) %>%
arrange(n)
nrc_data %>%
group_by(sentiment) %>%
summarise(n =n()) %>%
mutate(percent = n/sum(n)) %>%
arrange(n)
nrc_data %>%
group_by(sentiment) %>%
summarise(n=n()) %>%
mutate(percent = n/sum(n)) %>%
arrange(n)
nrc_data %>%
group_by(sentiment) %>%
summarise(n()) %>%
mutate(percent = n/sum(n)) %>%
arrange(n)
nrc_data %>%
group_by(sentiment) %>%
summarise(n=n()) %>%
mutate(percent = n/sum(n)) %>%
arrange(n)
nrc_data %>%
group_by(sentiment) %>%
summarise(n=n()) %>%
mutate(percent = n/sum(n)) %>%
arrange(n)
View(afinn_data)
source('~/src/text-analysis/main.R')
View(afinn_data)
View(bing_data)
View(nrc_data)
source('~/src/text-analysis/main.R')
install.packages("reticulate")
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = "/tmp")
library(tidyverse)  # data manipulation and visualization
library(modelr)     # provides easy pipeline modeling functions
library(broom)      # helps to tidy up model outputs
Advertising <- read.csv("~/Downloads/Advertising.csv")
View(Advertising)
Advertising <- read_csv("Users/gilbertking/src/regression-analysis/data/Advertising.csv")
Advertising <- read_csv("/Users/gilbertking/src/regression-analysis/data/Advertising.csv")
# remember that you will need to change the file location to match the location where you have stored the Advertising.csv and your Rmd file.
glimpse(Advertising)
set.seed(123) #set.seed is a way to fix a random sample rather than create a new sample each time you execute sample()
sample <- sample(c(TRUE, FALSE), nrow(Advertising), replace = T, prob = c(0.6,0.4))
train <- Advertising[sample, ]
test <- Advertising[!sample, ]
model1 <- lm(sales ~ TV, data = train)
summary(model1)
df <- augment(model1)
df
train$predicted <- df$.fitted
ggplot(train, aes(x = TV, y = sales)) + geom_point(color ="red") +geom_smooth(method = "lm", se=FALSE)+ geom_segment(aes(xend = TV, yend = predicted), alpha= .4) +theme_bw()
#ggplot(df, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0, linetype = "dashed", color = "red")
tidy(model1)
confint(model1)
sigma(model1)
sigma(model1)/mean(train$sales)
rsquare(model1, data = train)
cor(train$TV, train$sales)^2
ggplot(train, aes(TV, sales)) +
geom_point() +
geom_smooth(method = "lm") +
geom_smooth(se = FALSE, color = "red")
model1_results <- augment(model1, train)
ggplot(model1_results, aes(.fitted, .resid)) +
geom_ref_line(h = 0) +
geom_point() +
geom_smooth(se = FALSE) +
ggtitle("Residuals vs Fitted")
p1 <- ggplot(model1_results, aes(.fitted, .std.resid)) +
geom_ref_line(h = 0) +
geom_point() +
geom_smooth(method = "loess",se = FALSE) +
ggtitle("Standardized Residuals vs Fitted")
p2 <- ggplot(model1_results, aes(.fitted, sqrt(.std.resid))) +
geom_ref_line(h = 0) +
geom_point() +
geom_smooth(method = "loess",se = FALSE) +
ggtitle("Scale-Location")
gridExtra::grid.arrange(p1, p2, nrow = 1)
qq_plot <- qqnorm(model1_results$.resid) #this puts the dots on the chart
qq_plot <- qqline(model1_results$.resid) #this puts the line on the chart
par(mfrow=c(1, 2))
plot(model1, which = 4, id.n = 5)
plot(model1, which = 5, id.n = 5)
model1_results %>%
top_n(5, wt = .cooksd)
(test <- test %>%
add_predictions(model1))
# test MSE
test %>%
add_predictions(model1) %>%
summarise(MSE = mean((sales - pred)^2))
## # A tibble: 1 × 1
##        MSE
##      <dbl>
## MSE for test = 11.34993
# training MSE
train %>%
add_predictions(model1) %>%
summarise(MSE = mean((sales - pred)^2))
## # A tibble: 1 × 1
##        MSE
##      <dbl>
## MSE for train = 10.09814
model2 <- lm(sales ~ radio, data = train)
summary(model2)
train2 <- train
df2 <- augment(model2)
df2
train2$predicted <- df2$.fitted
ggplot(train2, aes(x = radio, y = sales)) + geom_point(color ="red") +geom_smooth(method = "lm", se=FALSE)+ geom_segment(aes(xend = radio, yend = predicted), alpha= .4) +theme_bw()
#ggplot(df, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0, linetype = "dashed", color = "red")
sigma(model2) #remember you must edit this to get the value for model 2
model2_results <- augment(model2, train)
p1 <- ggplot(model2_results, aes(.fitted, .std.resid)) +
geom_ref_line(h = 0) +
geom_point() +
geom_smooth(method = "loess",se = FALSE) +
ggtitle("Standardized Residuals vs Fitted")
p2 <- ggplot(model2_results, aes(.fitted, sqrt(.std.resid))) +
geom_ref_line(h = 0) +
geom_point() +
geom_smooth(method = "loess",se = FALSE) +
ggtitle("Scale-Location")
gridExtra::grid.arrange(p1, p2, nrow = 1)
#Creates a Summary of all the sales information.
summary(mysales)
knitr::opts_chunk$set(echo = TRUE)
mysales <- read.csv("/Users/gilbertking/src/MGT-585-Group-Project/salesData.csv")
mysales <- read.csv("/Users/gilbertking/src/MGT-585-Group-Project/data/salesData.csv")
sales <- read.table("file.txt", header = TRUE)
mysales <- read.csv("/Users/gilbertking/src/MGT-585-Group-Project/data/salesData.csv")
#sales <- read.table("file.txt", header = TRUE)
summary(mysales)
summary(mysales$Total.Profit)
summary(mysales$Total.Profit, mysales$Region)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
salesData <- read_csv("src/MGT-585-Group-Project/data/salesData.csv")
#Creates a Summary of all the sales information.
summary(mysales)
knitr::opts_chunk$set(echo = TRUE)
mysales <- read.csv("/Users/gilbertking/src/MGT-585-Group-Project/data/salesData.csv")
#sales <- read.table("file.txt", header = TRUE)
summary(mysales)
summary(mysales$Total.Profit, mysales$Region)
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
names(cdc)
summary(cdc)
summary(cdc$weight)
190 - 140
max(mysales$Total.Profit)
table(cdc$smoke100)
dim(cdc)
boxplot(cdc$height)
mysales <- read.csv("/Users/gilbertking/src/MGT-585-Group-Project/data/salesData.csv")
#sales <- read.table("file.txt", header = TRUE)
summary(mysales)
max(mysales$Total.Profit, mysales$Region)
knitr::opts_chunk$set(echo = TRUE)
max(mysales$Total.Profit, mysales$Region, xlab="Profit", ylab="Region")
plot(mysales$Total.Profit, mysales$Region, xlab="Profit", ylab="Region")
barplot(mysales$Total.Profit, mysales$Region, xlab="Region", ylab="Profit")
plot(mysales$Total.Profit, mysales$Region, xlab="Profit", ylab="Region")
shiny::runApp('src/salesly/server')
runApp('src/salesly/server')
runApp('src/salesly/server')
